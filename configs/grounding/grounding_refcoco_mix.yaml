train_file: ['/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcocog_umd.json', 
              '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcoco+_unc.json',
              '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcoco_unc.json']


val_file: {
          'g_val': '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcocog_umd.json',
          # 'g_test': '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcocog_umd.json',
          # 'coco+_val': '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcoco+_unc.json',
          # 'coco+_testA': '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcoco+_unc.json',
          # 'coco+_testB': '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcoco+_unc.json',
          # 'coco_val': '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcoco_unc.json',
          # 'coco_testA': '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcoco_unc.json',
          # 'coco_testB': '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcoco_unc.json',
}

test_file: {
          'g_val': '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcocog_umd.json',
          # 'g_test': '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcocog_umd.json',
          # 'coco+_val': '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcoco+_unc.json',
          # 'coco+_testA': '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcoco+_unc.json',
          # 'coco+_testB': '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcoco+_unc.json',
          # 'coco_val': '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcoco_unc.json',
          # 'coco_testA': '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcoco_unc.json',
          # 'coco_testB': '/mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcoco_unc.json',
}


image_root: '/mnt_rela/wangyabing.wyb/datasets/mscoco/all_pics'


loss_bbox_weight: 3
loss_giou_weight: 1

bridger_stages: [3,5,7,9,11]
aggregate_layers: [3,9] #defaultly include the last layer , i.e., 11-layer  ||  vit32 [3,9] vit16[3,7]
num_reg: 3

vit_type: vit_32 #vit_16
text_encoder: clip 

# swin+bert
# vit_type: swin 
# text_encoder: bert  


use_clip_vit: False
image_res: 640 #512 #224
patch_size: 32
hidden_dim: 512
vision_width: 256
text_width: 512


batch_size_test: 32
# temp: 0.07

optimizer: {opt: adamW, lr: 1e-4, weight_decay: 0.01, lr_mult: 2}
schedular: {sched: linear, lr: 1e-4, epochs: 10, num_warmup_steps: 0.1}


