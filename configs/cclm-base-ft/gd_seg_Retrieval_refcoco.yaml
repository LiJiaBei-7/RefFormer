dataset: refcoco
train_lmdb: /mnt_rela/wangyabing.wyb/datasets/vg_datasets/lmdb/refcoco/train_full.lmdb
train_split: train
# val_lmdb: /mnt_rela/wangyabing.wyb/datasets/vg_datasets/lmdb/refcoco/val_full.lmdb
# val_split: val

val_lmdb: /mnt_rela/wangyabing.wyb/datasets/vg_datasets/lmdb/refcoco/testA_full.lmdb
val_split: testA

mask_root: /mnt_rela/wangyabing.wyb/datasets/vg_datasets/refcoco/masks/refcoco
test_split: testA
test_lmdb: /mnt_rela/wangyabing.wyb/datasets/vg_datasets/lmdb/refcoco/testA_full.lmdb



maskId2bbox: /mnt_rela/wangyabing.wyb/datasets/vg_datasets/anns/refcoco_maskId2bbox.json

loss_bbox_weight: 3
loss_giou_weight: 1

vit_type: vit_16
bridger_stages: [3,5,7,9,11]
aggregate_layers: [3,9]
num_reg: 3


## Vision Encoder
vision_config: 'configs/config_swinB_384.json'
# vision_config: '/mnt/workspace/code_space/VG/CCLM/configs/config_clip_base_16.json'
# clip_config: 'ViT-B/16'
# clip_config: '/mnt/workspace/VisualSearch/clip_data/ViT-B-16.pt'

text_encoder: clip #robert

use_clip_vit: False
image_res: 640 #512 #224
patch_size: 32
hidden_dim: 512
vision_width: 256
text_width: 512

use_swin: True
# image_res: 384
# patch_size: 32

## Text Encoder (& Cross Encoder)
# text_encoder: 'data/xlm-roberta-large'
text_num_hidden_layers: 12



## Training
use_one_cl_proj_only: False

batch_size_train: 16
batch_size_test: 64
batch_size_test_text: 64
max_tokens: 40
embed_dim: 512
temp: 0.07
k_test: 128
num_queries: 5

optimizer: {opt: adamW, lr: 1e-4, weight_decay: 0.01, lr_mult: 2}
schedular: {sched: linear, lr: 1e-4, epochs: 10, num_warmup_steps: 0.1}

